{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach for Making Boolean Retrival Model \n",
    "#1.Preprocessing -Tokenization -Removal of Stop Words -Stemming.\n",
    "#2.Inverted-Positional Index - Posting Lists\n",
    "#3.Query Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import nltk #used for Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declarations - Lists & Dictionaries. \n",
    "inverted_index = {}\n",
    "tokens_ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer() #importing Porter Stemmer from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Preprocessing. \n",
    "\n",
    "# Importing all the stop words from the given file. \n",
    "# Tokenization of the given corpus. \n",
    "# Removal of Stopwords from the tokens, lowering them and then stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as', 'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up', 'his', 'her', 'in', 'on', 'no', 'we', 'do']\n"
     ]
    }
   ],
   "source": [
    "# Step1. Importing all the stopwords from the file for the removal. \n",
    "\n",
    "with open('Stopword-List.txt','r') as lines:\n",
    "    stopwords = lines.readlines()\n",
    "    stopwords = [x.rstrip() for x in stopwords]\n",
    "    print(stopwords)\n",
    "    \n",
    "# with-open automatically closes the file. \n",
    "# stop words are read and then stripped of, stored in stopwords.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alon',\n",
       " 'alon',\n",
       " 'along',\n",
       " 'away',\n",
       " 'beg',\n",
       " 'begin',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'bodi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step2. Tokenization - Removal of Stopwords \n",
    "\n",
    "for x in range(1,51):\n",
    "    myfile=open(\"ShortStories/\"+str(x)+ \".txt\",\"r\",encoding='utf-8')\n",
    "    #for x in tokens:\n",
    "    #print(x)\n",
    "    \n",
    "    words=myfile.read().replace(\".\",\"\").replace(\"n't\",\" not\").replace(\"'\",\"\").replace(\"]\",\" \").replace(\"[\",\"\").replace(\",\",\" \").replace(\"?\",\"\").replace(\"\\n\",\" \").replace(\"-\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"!\",\"\").replace(\"“\",\"\").replace(\"”\",\"\").replace(\":\",\"\").replace(\"*\",\"\").replace(\"—\",\"\").replace(\";\",\"\").replace(\"’\",\"\").split()\n",
    "    \n",
    "    #for i in range(len(temp)):\n",
    "    #temp[i]=[p_stemmer.stem(x.lower()) for x in temp[i]]\n",
    "    stem=[p_stemmer.stem(x.lower()) for x in words]\n",
    "    tokens = [x for x in stem if x not in stopwords]\n",
    "    inverted_index[x]=tokens\n",
    "    tokens_[x]=stem\n",
    "#len(tokens)\n",
    "#stem[0:20]\n",
    "tokens.sort()\n",
    "myfile.close()\n",
    "tokens[0:10]\n",
    "\n",
    "#inverted_index[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted_index\n",
    "#inverted_index.keys()\n",
    "#len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted - Positional Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted-Index\n",
    "index = {}\n",
    "\n",
    "for i in inverted_index.keys():\n",
    "    for j in set(inverted_index[i]):\n",
    "        if j not in index:\n",
    "            index[j]=[]\n",
    "            index[j].append(i)\n",
    "        else:\n",
    "            index[j].append(i)\n",
    "#index\n",
    "\n",
    "try:\n",
    "    file = open('InvertedIndex.txt', 'w')\n",
    "    file.write(str(index))\n",
    "    file.close()\n",
    "\n",
    "except:\n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Index \n",
    "\n",
    "pos_index={}\n",
    "for i in tokens_.keys():\n",
    "    count=0\n",
    "    for j in tokens_[i]:\n",
    "        count+=1\n",
    "        if j in stopwords:  # entertaining the presence of stop words in the file (increment index without doing anything)\n",
    "            continue\n",
    "        if j not in pos_index:\n",
    "            pos_index[j]={}\n",
    "            pos_index[j][i]=[]\n",
    "            pos_index[j][i].append(count)\n",
    "        else:\n",
    "            if i not in pos_index[j]:\n",
    "                pos_index[j][i]=[]\n",
    "            pos_index[j][i].append(count)\n",
    "\n",
    "\n",
    "#pos_index\n",
    "\n",
    "try:\n",
    "    file = open('PositionalIndex.txt', 'w')\n",
    "    file.write(str(pos_index))\n",
    "    file.close()\n",
    "\n",
    "except:\n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Processing  \n",
    "# Boolean Queries & Proximity Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1. \n",
    "#We have to take user input and stem that query to match the inverted-positional indexes for retrival "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_pos(word1, word2, prox_value,intersect): \n",
    "    result = []\n",
    "    for pos1 in word1:\n",
    "        for pos2 in word2:\n",
    "            if (abs(pos1-pos2)-1) == prox_value:\n",
    "                print(\"Document--> \",intersect,\"-->position1(\",pos1,\") - position2(\",pos2,\")\",abs(pos1-pos2)-1)\n",
    "                result.append(intersect)\n",
    "    return result\n",
    "\n",
    "def proximity_query(w1, w2, prox_value, pos_index):\n",
    "    word1={} \n",
    "    word2={}\n",
    "    for word in pos_index.keys():\n",
    "        if word == w1:\n",
    "            for i in pos_index[word]:\n",
    "                word1[i]=pos_index[word][i]\n",
    "        elif word==w2:\n",
    "            for i in pos_index[word]:\n",
    "                word2[i]=pos_index[word][i]\n",
    "                \n",
    "    s1=set(word1.keys())\n",
    "    s2=set(word2.keys())\n",
    "    intersect=None\n",
    "    intersect=s1.intersection(s2)\n",
    "    \n",
    "    finaldocs=[]\n",
    "    if intersect != None:\n",
    "        for intersect in intersect:\n",
    "                x=counter_pos(word1[intersect],word2[intersect], prox_value,intersect)\n",
    "                if len(x)>0:\n",
    "                    finaldocs.append(intersect)\n",
    "    else:\n",
    "        print(\"result not found\")\n",
    "        \n",
    "    print(\"\\nReturned Documents -->\",finaldocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_index(w,index):\n",
    "#     print(\"in word found func \")\n",
    "\n",
    "    for word in index.keys():\n",
    "        if word == w:\n",
    "            return set(index[word])\n",
    "        \n",
    "    return None\n",
    "\n",
    "def boolean_query(x, index): \n",
    "    \n",
    "    # For AND Query we use intersection \n",
    "    if 'and' in x:\n",
    "# matching if its AND Query.\n",
    "        val = [i for i in x if i=='and']\n",
    "        #print(val)\n",
    "    #The approach im using here is to determine number of AND's , as the complexity is 3 AND.\n",
    "        if len(val)==1: \n",
    "            count1=check_index(x[0],index)\n",
    "            count2=check_index(x[2],index)\n",
    "            count=count1.intersection(count2)\n",
    "            print(\"\\nReturned Documents -->\",count)\n",
    "        elif len(val)==2:\n",
    "            count1=check_index(x[0],index)\n",
    "            count2=check_index(x[2],index)\n",
    "            count3=check_index(x[4],index)\n",
    "            count=count3.intersection(count1.intersection(count2))\n",
    "            print(\"\\nReturned Documents -->\",count)\n",
    "        elif len(val)==3:\n",
    "            count1=check_index(x[0],index)\n",
    "            count2=check_index(x[2],index)\n",
    "            count3=check_index(x[4],index)\n",
    "            count4=check_index(x[8],index)\n",
    "            count=count4.intersection(count3.intersection(count1.intersection(count2)))\n",
    "            print(\"\\nReturned Documents -->\",count)\n",
    "\n",
    "    # For OR Query we use union\n",
    "    if 'or' in x: # matching if its OR Query.\n",
    "        val = [i for i in x if i=='or']\n",
    "        #print(val)\n",
    "    #The approach im using here is to determine number of OR's , as the complexity is 3 AND.\n",
    "        if len(val)==1: \n",
    "            count1=check_index(x[0],index)\n",
    "            count2=check_index(x[2],index)\n",
    "            count=count1.union(count2)\n",
    "            print(\"\\nReturned Documents -->\",count)\n",
    "        elif len(val)==2:\n",
    "            count1=check_index(x[0],index)\n",
    "            count2=check_index(x[2],index)\n",
    "            count3=check_index(x[4],index)\n",
    "            count=count3.union(count1.union(count2))\n",
    "            print(\"\\nReturned Documents -->\",count)\n",
    "        elif len(val)==3:\n",
    "            count1=check_index(x[0],index)\n",
    "            count2=check_index(x[2],index)\n",
    "            count3=check_index(x[4],index)\n",
    "            count4=check_index(x[8],index)\n",
    "            count=count4.union(count3.union(count1.union(count2)))\n",
    "            print(\"\\nReturned Documents -->\",count)\n",
    "                  \n",
    "    if 'not' in x:\n",
    "        j=set(tokens_.keys())\n",
    "        val=[i for i in x if i=='not']\n",
    "        if len(val)%2 == 0:\n",
    "             for word in index.keys():\n",
    "                    if word == x[len(val)]:\n",
    "                        print(index[word])\n",
    "        else:\n",
    "            for word in index.keys():\n",
    "                    if word == x[len(val)]:\n",
    "                        found=set(index[word])\n",
    "                        notfound=j.difference(found)\n",
    "                        print(\"\\nReturned Documents -->\",notfound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word_query(query,pos_index):\n",
    "    if len(query)==1: #confirming that it is one word\n",
    "        for word in pos_index.keys():\n",
    "            if word == query[0]: #iteration over to find the word\n",
    "                check = []\n",
    "                for i in pos_index[word]:\n",
    "                    check.append(i)  #appening the num of docs into check list. \n",
    "        print(\"Returned Documents:\", check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ENTER QUERY FOR SEARCHING:  beard\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beard']\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_query(query):\n",
    "    #Preprocessing of the Query. \n",
    "    query=query.replace(\".\",\"\").replace(\"n't\",\" not\").replace(\"'\",\"\").replace(\"]\",\" \").replace(\"[\",\"\").replace(\",\",\" \").replace(\"?\",\"\").replace(\"\\n\",\" \").replace(\"-\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"!\",\"\").replace(\"“\",\"\").replace(\"”\",\"\").replace(\":\",\"\").replace(\"/\",\" / \").replace(\"*\",\"\").replace(\"—\",\"\").replace(\";\",\"\").replace(\"’\",\"\").split()\n",
    "    stem_query=[p_stemmer.stem(x.lower()) for x in query]\n",
    "    return stem_query\n",
    "\n",
    "\n",
    "query = input(\"ENTER QUERY FOR SEARCHING: \")\n",
    "x =preprocessing_query(query)\n",
    "print(x)\n",
    "\n",
    "#x[0] - picking the first splited word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned Documents: [1, 2, 4, 6, 11, 20, 21, 23, 25, 26, 31, 34, 40, 44]\n"
     ]
    }
   ],
   "source": [
    "#Boolean Queries\n",
    "if 'and' in x or 'or' in x or 'not' in x: \n",
    "    boolean_query(x,index)\n",
    "\n",
    "#Proximity Queries\n",
    "elif '/' in x:\n",
    "    proximity= int(x[x.index('/')+1])\n",
    "    w1=x[0]\n",
    "    w2=x[1]\n",
    "    if proximity != 0:\n",
    "        proximity_query(w1,w2,proximity,pos_index)\n",
    "#One word queries.      \n",
    "elif 'not' not in x and 'and' not in x and 'or' not in x:\n",
    "    if len(x)!=0:\n",
    "        one_word_query(x,pos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
